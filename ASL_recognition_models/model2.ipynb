{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d73ba-eb9a-442e-b53f-c372f175a231",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#Kfold no. 1, 2, 3, 4 only\n",
    "num = \n",
    "\n",
    "#Dataset. \"YoutubeDataset10\" or \"CustomDataset10.\" without the quotation marks(\"\")\n",
    "dataset = \n",
    "\n",
    "model = 'efficientNetV2S+LSTM'\n",
    "\n",
    "train_path = f'/{dataset}_TRAIN_KFOLD{num}'\n",
    "val_path = f'/{dataset}_VAL_KFOLD{num}'\n",
    "\n",
    "train_dataset_path = os.listdir(train_path)\n",
    "val_dataset_path = os.listdir(val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a604a6-fb48-45ad-bec4-023060c683df",
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms = []\n",
    "\n",
    "for item in train_dataset_path:\n",
    " # Get all the file names\n",
    " all_rooms = os.listdir(train_path + '/' +item)\n",
    "\n",
    " # Add them to the list\n",
    " for room in all_rooms:\n",
    "    rooms.append((item, str(train_path + '/' +item) + '/' + room))\n",
    "    \n",
    "# Build a dataframe        \n",
    "train_dataframe = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
    "print(train_dataframe.head())\n",
    "print(train_dataframe.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412037e0-b40c-4684-afc4-c32c43902112",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_dataframe.loc[:,['video_name','tag']]\n",
    "train_df\n",
    "train_df.to_csv(f'youtube_{model}_train_dataframe_KFOLD{num}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c628c9f-64ac-45b4-9517-fa3fd06361e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms = []\n",
    "\n",
    "for item in val_dataset_path:\n",
    " # Get all the file names\n",
    " all_rooms = os.listdir(val_path + '/' +item)\n",
    "\n",
    " # Add them to the list\n",
    " for room in all_rooms:\n",
    "    rooms.append((item, str(val_path + '/' +item) + '/' + room))\n",
    "    \n",
    "# Build a dataframe        \n",
    "val_dataframe = pd.DataFrame(data=rooms, columns=['tag', 'video_name'])\n",
    "print(val_dataframe.head())\n",
    "print(val_dataframe.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247fcded-b3ed-43e2-afd1-e822ab1316ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = val_dataframe.loc[:,['video_name','tag']]\n",
    "val_df\n",
    "val_df.to_csv(f'youtube_{model}_val_dataframe_KFOLD{num}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eeecae-e0f5-480e-a717-a758054db3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import imageio\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69406d1e-cae2-4d76-840c-55d57fd88c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],[tf.config.experimental.VirtualDeviceConfiguration(memory_limit=8120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc19c589-594b-447e-844d-8608e01b3014",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923f2c9b-15e6-440b-bc0b-a878c822203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d4551e-94a8-4164-b128-5bc6bbca277f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5f9756-f0a6-4101-a490-d80d24663cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c54768f-71e2-495e-a18c-0da8f6132f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(f'youtube_{model}_train_dataframe_KFOLD{num}.csv')\n",
    "\n",
    "\n",
    "print(f\"Total videos in train_df: {len(train_df)}\")\n",
    "\n",
    "\n",
    "\n",
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931422bc-c9ba-4470-ae87-5a65115df158",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df = pd.read_csv(f'youtube_{model}_val_dataframe_KFOLD{num}.csv')\n",
    "\n",
    "\n",
    "print(f\"Total videos in val_df: {len(val_df)}\")\n",
    "\n",
    "\n",
    "\n",
    "val_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d952a2d-0a23-4630-a62f-71dcf56e7c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following two methods are taken from this tutorial:\n",
    "# https://www.tensorflow.org/hub/tutorials/action_recognition_with_tf_hub\n",
    "#APPLY YOUR OWN PREPROCESSING TECHNIQUE AS NEED BE\n",
    "#CROPPING SPECIFICATIONS IS SPECIFIC FOR CustomDataset10\n",
    "IMG_SIZE = 448\n",
    "NUM_FRAMES = 30\n",
    "NUM_CHANNELS = 3\n",
    "\n",
    "\n",
    "def crop_center_square(frame):\n",
    "    y, x = frame.shape[0:2]\n",
    "    min_dim = min(y, x)\n",
    "    start_x = (x // 2) - (min_dim // 2)\n",
    "    start_y = (y // 2) - (min_dim // 2)\n",
    "    return frame[start_y : start_y + min_dim, start_x : start_x + min_dim]\n",
    "\n",
    "\n",
    "def load_video(path, max_frames=NUM_FRAMES, resize=(IMG_SIZE, IMG_SIZE)):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    frames = []\n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = crop_center_square(frame)         \n",
    "            frame = cv2.resize(frame, resize)\n",
    "            frame = frame[112:336, 112:336]\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "            frame = frame[ :, :, [2, 1, 0]]\n",
    "            frames.append(frame)\n",
    "            if len(frames) == max_frames:\n",
    "                break\n",
    "    finally:\n",
    "        cap.release()\n",
    "    return np.array(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dec3c0-a0c5-4eca-9185-a8f7d0a25655",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train\n",
    "label_processor_train = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(train_df[\"tag\"]))\n",
    "labels_train = train_df[\"tag\"].values\n",
    "labels_train = label_processor_train(labels_train[..., None]).numpy()\n",
    "print(label_processor_train.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b04fb3-751b-414b-b161-ccdb9464b013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val\n",
    "label_processor_val = keras.layers.StringLookup(num_oov_indices=0, vocabulary=np.unique(val_df[\"tag\"]))\n",
    "labels_val = val_df[\"tag\"].values\n",
    "labels_val = label_processor_val(labels_val[..., None]).numpy()\n",
    "print(label_processor_val.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d88239-349c-4a8b-983e-f9c8aefd0e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train_df = shuffle(train_df)\n",
    "val_df = shuffle(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e8fe75-81c0-4ef7-af6c-c4f103448942",
   "metadata": {},
   "outputs": [],
   "source": [
    "i3d_input_size = 224\n",
    "\n",
    "def build_feature_extractor():\n",
    "    feature_extractor = tf.keras.applications.EfficientNetV2S(\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        pooling=\"avg\",\n",
    "        input_shape=(i3d_input_size, i3d_input_size, 3),\n",
    "    )\n",
    "    preprocess_input = tf.keras.applications.efficientnet_v2.preprocess_input\n",
    "\n",
    "    inputs = keras.Input((i3d_input_size, i3d_input_size, 3))\n",
    "    preprocessed = preprocess_input(inputs)\n",
    "    outputs = feature_extractor(preprocessed)\n",
    "    return keras.Model(inputs, outputs, name=\"feature_extractor\")\n",
    "\n",
    "feature_extractor = build_feature_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfd8d1c-3df0-4bf3-b764-09a7926d7c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 30\n",
    "NUM_FEATURES = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7799ff5c-e8f1-4326-b9fd-67576a7c5dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_all_videos(df):\n",
    "  \n",
    "    num_samples = len(df)\n",
    "    video_paths = df[\"video_name\"].values.tolist()\n",
    "    labels = df[\"tag\"].values\n",
    "    \n",
    "    #convert classlabels to label encoding\n",
    "    labels = label_processor_train(labels[..., None]).numpy()\n",
    "    print(len(labels))\n",
    "\n",
    "    frame_features = np.zeros(shape=(num_samples, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\") #145,20,2048\n",
    "\n",
    "    # For each video.\n",
    "    for idx, path in enumerate(video_paths):\n",
    "        print(\"Video number: \", idx)\n",
    "        print(\"Path:         \", path)\n",
    "        # Gather all its frames and add a batch dimension.\n",
    "        frames = load_video(path)\n",
    "        frames = frames[None, ...]\n",
    "        # Initialize placeholders to store the masks and features of the current video.\n",
    "        #temp_frame_mask = np.zeros(shape=(1, MAX_SEQ_LENGTH,), dtype=\"bool\")\n",
    "        temp_frame_features = np.zeros(\n",
    "            shape=(1, MAX_SEQ_LENGTH, NUM_FEATURES), dtype=\"float32\"\n",
    "        )\n",
    "\n",
    "        # Extract features from the frames of the current video.\n",
    "        for i, batch in enumerate(frames):\n",
    "            video_length = batch.shape[0]\n",
    "            length = min(MAX_SEQ_LENGTH, video_length)\n",
    "            print(\"length:  \\n\", length)\n",
    "            \n",
    "            for j in range(length):\n",
    "                temp_frame_features[i, j, :] = feature_extractor.predict(\n",
    "                    batch[None, j, :], verbose=0\n",
    "                )\n",
    "        frame_features[idx,] = temp_frame_features.squeeze()\n",
    "        \n",
    "    return frame_features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20656580-fdbe-49ad-b91b-14473aa061cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_data, train_labels = prepare_all_videos(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d623b-a2ce-4cda-8cdd-cbd06a4e447d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_data, val_labels = prepare_all_videos(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfed9d3-816e-47c5-9bc1-1e90b82b59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Train Data Shape: {train_data.shape}\")\n",
    "print(f\"Train Labels Shape: {train_labels.shape}\")\n",
    "print(f\"Val Data Shape: {val_data.shape}\")\n",
    "print(f\"Val Labels Shape: {val_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55310530-34cf-4066-8f92-6dec72a3b57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_learning_rate = 0.0001\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate,\n",
    "    decay_steps=100000,\n",
    "    decay_rate=0.96,\n",
    "    staircase=False)\n",
    "\n",
    "# LearningRate = InitialLearningRate * Droprate ^ (Epoch/EpochDrop)\n",
    "# LearningRate = initialLearningRate * decay_rate ^ (step/decay_steps)\n",
    "# step is best if it is proportionate with the global_step of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b714fd8-6580-488d-9c6d-4353038c81a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility for our sequence model.\n",
    "def get_sequence_model():\n",
    "    class_vocab = label_processor_train.get_vocabulary()\n",
    "    len(class_vocab)\n",
    "    frame_features_input = keras.Input((MAX_SEQ_LENGTH, NUM_FEATURES))\n",
    "\n",
    "    x = keras.layers.LSTM(512)(\n",
    "        frame_features_input\n",
    "    )    \n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "    x = keras.layers.Dense(1024, activation=\"relu\")(x)\n",
    "    x = keras.layers.Dropout(0.25)(x)\n",
    "    output = keras.layers.Dense(len(class_vocab), activation=\"softmax\")(x)\n",
    "    rnn_model = keras.Model(frame_features_input, output)\n",
    "    rnn_model.compile(\n",
    "        loss=\"sparse_categorical_crossentropy\", optimizer=keras.optimizers.Adam(learning_rate=lr_schedule), metrics=[\"accuracy\"]\n",
    "    )\n",
    "    return rnn_model\n",
    "seq_model = get_sequence_model()\n",
    "seq_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f77d34-84d1-4105-ae90-0ce901d8fd3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility for running experiments.\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=3,\n",
    ")\n",
    "\n",
    "callbacks = [early_stopping]\n",
    "\n",
    "seq_model = get_sequence_model()\n",
    "history = seq_model.fit(\n",
    "    train_data,\n",
    "    train_labels,\n",
    "    validation_data = (val_data, val_labels),\n",
    "    epochs=100,\n",
    "    batch_size=16,\n",
    "    callbacks=[callbacks],\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20f8ec6-c9ad-4bd6-8999-df857d0720ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = seq_model.predict(x=val_data, batch_size=16, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04209e20-be63-4123-91d9-c5f00c43a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "rounded_predictions = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3bf495-9084-4777-b39a-233b8c07dbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a5c49-7d00-4748-b91a-81041408022f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b95cb-5e2d-48af-8895-07d2a1b6ac52",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true = val_labels, y_pred = rounded_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9020cddf-1635-4735-a38b-bdb398ad30b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap = plt.cm.Blues):\n",
    "    \n",
    "    plt.imshow(cm, interpolation = 'nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix but withoout Normalization\")\n",
    "        \n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cba590-ddaf-4c93-b219-5b5291eb3f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_plot_labels = [ 'Food', 'Hello', 'HowAreYou', 'Internet', 'Like', 'Ready', 'Sad', 'Talk', 'ThankYou', 'Yes' ]\n",
    "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='confusion matrix')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
